{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tous les imports sont ici."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\tomge/train_dataset\\\\metadata.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 65\u001b[0m\n\u001b[0;32m     60\u001b[0m         label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvideo_files[idx]]\n\u001b[0;32m     62\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m video, label\n\u001b[1;32m---> 65\u001b[0m dataset\u001b[38;5;241m=\u001b[39m\u001b[43mTrainVideoDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpanduser\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m~/train_dataset\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[3], line 41\u001b[0m, in \u001b[0;36mTrainVideoDataset.__init__\u001b[1;34m(self, root_dir)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, root_dir):\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot_dir \u001b[38;5;241m=\u001b[39m root_dir\n\u001b[1;32m---> 41\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[0;32m     42\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(file)\n\u001b[0;32m     43\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m {k : (torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;241m1\u001b[39m)) \u001b[38;5;28;01mif\u001b[39;00m v \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFAKE\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;241m0\u001b[39m))) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mitems()}\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\interactiveshell.py:286\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    279\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    280\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    281\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    282\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    283\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    284\u001b[0m     )\n\u001b[1;32m--> 286\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\tomge/train_dataset\\\\metadata.json'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.io as io\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import timm\n",
    "import wandb\n",
    "\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deux fonctions utiles :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_data(data, new_height, new_width, x=0, y=0, height=None, width=None):\n",
    "    '''\n",
    "    Data est un tenseur de dimension (..., channels, image_height, image_width)\n",
    "    Retourne un tensor de dimension (..., channels, new_height, new_width)\n",
    "    Possibilité de définir un cadre de coin haut gauche (x,y) de dimension (height, width) pour cropper les images contenues dans Data\n",
    "    (padding si le cadre est trop grand)\n",
    "    Voir les exemples (situés après la création du dataset)\n",
    "    '''\n",
    "    full_height = data.shape[-2]\n",
    "    full_width = data.shape[-1]\n",
    "    height = full_height - y if height is None else height\n",
    "    width = full_width -x if width is None else width\n",
    "    \n",
    "\n",
    "    ratio = new_height/new_width\n",
    "    if height/width > ratio:\n",
    "        expand_height = height\n",
    "        expand_width = int(height / ratio)\n",
    "    elif height/width < ratio:\n",
    "        expand_height = int(width * ratio)\n",
    "        expand_width = width\n",
    "    else:\n",
    "        expand_height = height\n",
    "        expand_width = width\n",
    "    tr = transforms.Compose([\n",
    "        transforms.CenterCrop((expand_height, expand_width)),\n",
    "        transforms.Resize((new_height, new_width))\n",
    "    ])\n",
    "    return tr(data[...,y:min(y+height, full_height), x:min(x+width, full_width)])\n",
    "\n",
    "def display_image(img) :\n",
    "    '''\n",
    "    affiche l'image img (img est un tenseur)\n",
    "    '''\n",
    "    img = img.permute(1,2,0)\n",
    "    plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Création du dataset. On ne garde que nb_frames d'images par vidéo, et on traite les vidéos image par image.\n",
    "\n",
    "ATTENTION : les images des vidéos sont déjà resized en 256x256, vous pouvez bien sûr modifier cette taille selon ce que prend votre modèle en entrée."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_frames = 10\n",
    "\n",
    "class TrainVideoDataset(Dataset):\n",
    "    def __init__(self, root_dir):\n",
    "        self.root_dir = root_dir\n",
    "        with open(os.path.join(root_dir, \"metadata.json\"), 'r') as file:\n",
    "            self.data= json.load(file)\n",
    "            self.data = {k : (torch.tensor(float(1)) if v == 'FAKE' else torch.tensor(float(0))) for k, v in self.data.items()}\n",
    "        self.video_files = [f for f in os.listdir(root_dir) if f.endswith('.mp4') and f in self.data.keys()]\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.video_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        video_path = os.path.join(self.root_dir, self.video_files[idx])\n",
    "        video, audio, info = io.read_video(video_path, pts_unit='sec')\n",
    "\n",
    "        video = video.permute(0,3,1,2)\n",
    "        length = video.shape[0]\n",
    "        video = video[[i*(length//(nb_frames-1)) for i in range(nb_frames)]]\n",
    "        \n",
    "        video = torch.stack([resize_data(img, 256, 256)/255 for img in video])\n",
    "\n",
    "        label = self.data[self.video_files[idx]]\n",
    "        \n",
    "        return video, label\n",
    "\n",
    "    \n",
    "dataset=TrainVideoDataset(os.path.expanduser(\"~/train_dataset\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exemple pour la fonction resize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video, label = dataset[0]\n",
    "img=video[0]\n",
    "\n",
    "print(img.shape)\n",
    "img=resize_data(img, 220 , 300, 50, 40, 200, 190)\n",
    "display_image(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepfakeDetector(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.dense=nn.Linear(nb_frames*3*256*256,1)\n",
    "        self.flat=nn.Flatten()\n",
    "        self.norm=nn.BatchNorm1d(1)\n",
    "    def forward(self, x):\n",
    "        y=self.flat(x)\n",
    "        y=self.dense(y)\n",
    "        y=self.norm(y)\n",
    "        return (y+1)/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boucle d'entraînement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "loss_fn = nn.MSELoss()\n",
    "model = DeepfakeDetector().to(device)\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "nb_epochs = 5\n",
    "batch_size = 2\n",
    "\n",
    "\n",
    "\n",
    "run = wandb.init(\n",
    "    project=\"automathon\",\n",
    "    name=\"nom-de-votre-equipe\",\n",
    "    config={\n",
    "        \"learning_rate\": 0.001,\n",
    "        \"architecture\": \"-\",\n",
    "        \"dataset\": \"DeepFake Detection Challenge\",\n",
    "        \"epochs\": 10,\n",
    "        \"batch_size\": 10,\n",
    "    },\n",
    ")\n",
    "\n",
    "\n",
    "loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "for epoch in range(nb_epochs) :\n",
    "    pbar = tqdm(loader, desc=\"Epoch {}\".format(epoch), ncols=0)\n",
    "    for sample in pbar:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        X, label = sample\n",
    "        \n",
    "        X = X.to(device)\n",
    "        label = label.to(device)\n",
    "        label_pred = model(X)\n",
    "        label=torch.unsqueeze(label,dim=1)\n",
    "        loss = loss_fn(label, label_pred)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        run.log({\"training_loss\": loss.item()}, step=epoch)\n",
    "        print(f\"Loss {loss.item():.4f}\")\n",
    "    pbar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boucle de test : crée la liste des prédictions de votre modèle sur les données de test.\n",
    "\n",
    "ATTENTION : on utilise un autre DataLoader (car on a pas accès aux labels pour test), où on resize également les images en 256x256."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TestVideoDataset(Dataset):\n",
    "    def __init__(self, root_dir):\n",
    "        self.root_dir = root_dir\n",
    "        self.video_files = [f for f in os.listdir(root_dir) if f.endswith('.mp4')]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.video_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        video_path = os.path.join(self.root_dir, self.video_files[idx])\n",
    "        video, audio, info = io.read_video(video_path, pts_unit='sec')\n",
    "\n",
    "        video = video.permute(0,3,1,2)\n",
    "        length = video.shape[0]\n",
    "        video = video[[i*(length//(nb_frames-1)) for i in range(nb_frames)]]\n",
    "        \n",
    "        video = torch.stack([resize_data(img, 256, 256)/255 for img in video])\n",
    "\n",
    "        return video\n",
    "\n",
    "\n",
    "test_data=TestVideoDataset(os.path.expanduser(\"~/test_dataset\"))\n",
    "\n",
    "test_loader=DataLoader(test_data, batch_size=batch_size, shuffle=False) \n",
    "\n",
    "predictions=[]\n",
    "\n",
    "for sample in test_loader :\n",
    "    X= sample\n",
    "    X = X.to(device)\n",
    "    label_pred = model(X)\n",
    "\n",
    "    predictions.append(label_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Création du fichier tests.csv qui contient vos prédictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_id_column_name = \"ID\"\n",
    "\n",
    "y_pred = pd.DataFrame(index= range(len(label_pred)))\n",
    "y_pred[\"TARGET\"]= pd.DataFrame(torch.detach(label_pred).numpy())\n",
    "print(y_pred)\n",
    "\n",
    "y_pred.to_csv(\"tests\", sep=',', index_label='ID')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
